{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps Final Project:\n",
    "\n",
    "Comparing performance of fine-tuned LLM vs out of the box pretrained LLM on sentiment analysis task for IMDB reviews dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/DistributedComputing/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae305988d104f6bbcdcbc3463189bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/799 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dadb0250117648f3a19cd9b4054af3d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "df = pd.read_csv('movie_short.csv')\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilBERT Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7824edc9ac31452991176ee88adb9a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7011, 'grad_norm': 1.1648861169815063, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.1}\n",
      "{'loss': 0.7, 'grad_norm': 1.393163800239563, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.2}\n",
      "{'loss': 0.6905, 'grad_norm': 1.2457865476608276, 'learning_rate': 3e-06, 'epoch': 0.3}\n",
      "{'loss': 0.692, 'grad_norm': 1.8890949487686157, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.4}\n",
      "{'loss': 0.6888, 'grad_norm': 1.626924991607666, 'learning_rate': 5e-06, 'epoch': 0.5}\n",
      "{'loss': 0.6994, 'grad_norm': 1.5356136560440063, 'learning_rate': 6e-06, 'epoch': 0.6}\n",
      "{'loss': 0.693, 'grad_norm': 2.3892295360565186, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.7}\n",
      "{'loss': 0.6826, 'grad_norm': 2.665776252746582, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.8}\n",
      "{'loss': 0.6768, 'grad_norm': 2.523048162460327, 'learning_rate': 9e-06, 'epoch': 0.9}\n",
      "{'loss': 0.6404, 'grad_norm': 1.8623087406158447, 'learning_rate': 1e-05, 'epoch': 1.0}\n",
      "{'loss': 0.6054, 'grad_norm': 2.162881374359131, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.1}\n",
      "{'loss': 0.5159, 'grad_norm': 3.9567172527313232, 'learning_rate': 1.2e-05, 'epoch': 1.2}\n",
      "{'loss': 0.4395, 'grad_norm': 11.126026153564453, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.3}\n",
      "{'loss': 0.4418, 'grad_norm': 7.44964599609375, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.4}\n",
      "{'loss': 0.3864, 'grad_norm': 15.361994743347168, 'learning_rate': 1.5e-05, 'epoch': 1.5}\n",
      "{'loss': 0.3683, 'grad_norm': 13.4083251953125, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.6}\n",
      "{'loss': 0.295, 'grad_norm': 22.17560386657715, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.7}\n",
      "{'loss': 0.3877, 'grad_norm': 17.629112243652344, 'learning_rate': 1.8e-05, 'epoch': 1.8}\n",
      "{'loss': 0.5426, 'grad_norm': 9.485539436340332, 'learning_rate': 1.9e-05, 'epoch': 1.9}\n",
      "{'loss': 0.3337, 'grad_norm': 4.116492748260498, 'learning_rate': 2e-05, 'epoch': 2.0}\n",
      "{'loss': 0.175, 'grad_norm': 0.7154840230941772, 'learning_rate': 2.1e-05, 'epoch': 2.1}\n",
      "{'loss': 0.1879, 'grad_norm': 31.07720184326172, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.2}\n",
      "{'loss': 0.0732, 'grad_norm': 0.3018510937690735, 'learning_rate': 2.3000000000000003e-05, 'epoch': 2.3}\n",
      "{'loss': 0.2202, 'grad_norm': 8.884821891784668, 'learning_rate': 2.4e-05, 'epoch': 2.4}\n",
      "{'loss': 0.1687, 'grad_norm': 1.3780461549758911, 'learning_rate': 2.5e-05, 'epoch': 2.5}\n",
      "{'loss': 0.3263, 'grad_norm': 19.90458869934082, 'learning_rate': 2.6000000000000002e-05, 'epoch': 2.6}\n",
      "{'loss': 0.5846, 'grad_norm': 45.10395812988281, 'learning_rate': 2.7000000000000002e-05, 'epoch': 2.7}\n",
      "{'loss': 0.1914, 'grad_norm': 18.501811981201172, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.8}\n",
      "{'loss': 0.3276, 'grad_norm': 0.3324565291404724, 'learning_rate': 2.9e-05, 'epoch': 2.9}\n",
      "{'loss': 0.2764, 'grad_norm': 45.32845687866211, 'learning_rate': 3e-05, 'epoch': 3.0}\n",
      "{'train_runtime': 163.9231, 'train_samples_per_second': 14.623, 'train_steps_per_second': 1.83, 'train_loss': 0.4570730966329575, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.4570730966329575, metrics={'train_runtime': 163.9231, 'train_samples_per_second': 14.623, 'train_steps_per_second': 1.83, 'total_flos': 317524354578432.0, 'train_loss': 0.4570730966329575, 'epoch': 3.0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaulating DistilBERT fine-tuned vs pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:14<00:00,  1.69it/s]\n",
      "/opt/homebrew/anaconda3/envs/DistributedComputing/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 25/25 [00:14<00:00,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Model - Accuracy: 0.86, F1 Score: 0.8494623655913979\n",
      "Pre-trained Model - Accuracy: 0.51, F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm  # Use standard tqdm\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataset, batch_size=8):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    preds = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):  \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels.extend(batch['label'].cpu().numpy())\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    return accuracy, f1\n",
    "\n",
    "finetuned_model = model\n",
    "finetuned_accuracy, finetuned_f1 = evaluate_model(finetuned_model, val_dataset)\n",
    "\n",
    "pretrained_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "pretrained_accuracy, pretrained_f1 = evaluate_model(pretrained_model, val_dataset)\n",
    "\n",
    "print(f'Fine-tuned Model - Accuracy: {finetuned_accuracy}, F1 Score: {finetuned_f1}')\n",
    "print(f'Pre-trained Model - Accuracy: {pretrained_accuracy}, F1 Score: {pretrained_f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning with LoRA instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/DistributedComputing/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e294e8f2938244a1aa8f7825f2470039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7221, 'grad_norm': 0.10963136702775955, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.1}\n",
      "{'loss': 0.7116, 'grad_norm': 0.1270284503698349, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.2}\n",
      "{'loss': 0.706, 'grad_norm': 0.10134343057870865, 'learning_rate': 3e-06, 'epoch': 0.3}\n",
      "{'loss': 0.7136, 'grad_norm': 0.1646464467048645, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.4}\n",
      "{'loss': 0.7533, 'grad_norm': 0.15595752000808716, 'learning_rate': 5e-06, 'epoch': 0.5}\n",
      "{'loss': 0.7051, 'grad_norm': 0.11388856917619705, 'learning_rate': 6e-06, 'epoch': 0.6}\n",
      "{'loss': 0.7031, 'grad_norm': 0.21470071375370026, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.7}\n",
      "{'loss': 0.6796, 'grad_norm': 0.13678190112113953, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.8}\n",
      "{'loss': 0.7245, 'grad_norm': 0.1396872103214264, 'learning_rate': 9e-06, 'epoch': 0.9}\n",
      "{'loss': 0.7288, 'grad_norm': 0.164378821849823, 'learning_rate': 1e-05, 'epoch': 1.0}\n",
      "{'loss': 0.7124, 'grad_norm': 0.12946459650993347, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.1}\n",
      "{'loss': 0.7122, 'grad_norm': 0.12616731226444244, 'learning_rate': 1.2e-05, 'epoch': 1.2}\n",
      "{'loss': 0.7205, 'grad_norm': 0.13493697345256805, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.3}\n",
      "{'loss': 0.738, 'grad_norm': 0.4659038782119751, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.4}\n",
      "{'loss': 0.719, 'grad_norm': 0.2536369860172272, 'learning_rate': 1.5e-05, 'epoch': 1.5}\n",
      "{'loss': 0.6887, 'grad_norm': 0.1932983249425888, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.6}\n",
      "{'loss': 0.7127, 'grad_norm': 0.1408839225769043, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.7}\n",
      "{'loss': 0.7147, 'grad_norm': 0.15335386991500854, 'learning_rate': 1.8e-05, 'epoch': 1.8}\n",
      "{'loss': 0.7098, 'grad_norm': 0.14273470640182495, 'learning_rate': 1.9e-05, 'epoch': 1.9}\n",
      "{'loss': 0.7027, 'grad_norm': 0.25989967584609985, 'learning_rate': 2e-05, 'epoch': 2.0}\n",
      "{'loss': 0.7008, 'grad_norm': 0.16816329956054688, 'learning_rate': 2.1e-05, 'epoch': 2.1}\n",
      "{'loss': 0.7014, 'grad_norm': 0.16126461327075958, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.2}\n",
      "{'loss': 0.7002, 'grad_norm': 0.38778114318847656, 'learning_rate': 2.3000000000000003e-05, 'epoch': 2.3}\n",
      "{'loss': 0.6992, 'grad_norm': 0.17109012603759766, 'learning_rate': 2.4e-05, 'epoch': 2.4}\n",
      "{'loss': 0.6999, 'grad_norm': 0.3580131530761719, 'learning_rate': 2.5e-05, 'epoch': 2.5}\n",
      "{'loss': 0.681, 'grad_norm': 0.291911244392395, 'learning_rate': 2.6000000000000002e-05, 'epoch': 2.6}\n",
      "{'loss': 0.7016, 'grad_norm': 0.2238336205482483, 'learning_rate': 2.7000000000000002e-05, 'epoch': 2.7}\n",
      "{'loss': 0.6923, 'grad_norm': 0.2899482548236847, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.8}\n",
      "{'loss': 0.7007, 'grad_norm': 0.4326309263706207, 'learning_rate': 2.9e-05, 'epoch': 2.9}\n",
      "{'loss': 0.6914, 'grad_norm': 0.48042505979537964, 'learning_rate': 3e-05, 'epoch': 3.0}\n",
      "{'train_runtime': 138.483, 'train_samples_per_second': 17.309, 'train_steps_per_second': 2.166, 'train_loss': 0.7082284053166708, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:16<00:00,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Model - Accuracy: 0.645, F1 Score: 0.6926406926406926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules= [\"attention.q_lin\", \"attention.k_lin\", \"attention.v_lin\"], #[\"query\", \"key\", \"value\"],  # Specify the target modules\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(lora_model, lora_config)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "lora_finetuned_model = lora_model\n",
    "lora_finetuned_accuracy, lora_finetuned_f1 = evaluate_model(lora_finetuned_model, val_dataset)\n",
    "\n",
    "print(f'Fine-tuned Model - Accuracy: {lora_finetuned_accuracy}, F1 Score: {lora_finetuned_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same process as above but on DistillRoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/DistributedComputing/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be88dfd49e9c4c589cb421ef5cb42ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6999, 'grad_norm': 1.3030778169631958, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.1}\n",
      "{'loss': 0.7006, 'grad_norm': 2.0900352001190186, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.2}\n",
      "{'loss': 0.6983, 'grad_norm': 1.2767045497894287, 'learning_rate': 3e-06, 'epoch': 0.3}\n",
      "{'loss': 0.6956, 'grad_norm': 1.7100857496261597, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.4}\n",
      "{'loss': 0.6823, 'grad_norm': 1.9790457487106323, 'learning_rate': 5e-06, 'epoch': 0.5}\n",
      "{'loss': 0.7201, 'grad_norm': 1.873080849647522, 'learning_rate': 6e-06, 'epoch': 0.6}\n",
      "{'loss': 0.7134, 'grad_norm': 3.24796986579895, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.7}\n",
      "{'loss': 0.7103, 'grad_norm': 3.5436837673187256, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.8}\n",
      "{'loss': 0.6854, 'grad_norm': 2.443457841873169, 'learning_rate': 9e-06, 'epoch': 0.9}\n",
      "{'loss': 0.6914, 'grad_norm': 2.5282208919525146, 'learning_rate': 1e-05, 'epoch': 1.0}\n",
      "{'loss': 0.692, 'grad_norm': 2.087980031967163, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.1}\n",
      "{'loss': 0.6833, 'grad_norm': 1.788851022720337, 'learning_rate': 1.2e-05, 'epoch': 1.2}\n",
      "{'loss': 0.6823, 'grad_norm': 1.3680816888809204, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.3}\n",
      "{'loss': 0.6848, 'grad_norm': 7.20403528213501, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.4}\n",
      "{'loss': 0.703, 'grad_norm': 6.570723533630371, 'learning_rate': 1.5e-05, 'epoch': 1.5}\n",
      "{'loss': 0.6937, 'grad_norm': 3.3013458251953125, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.6}\n",
      "{'loss': 0.6994, 'grad_norm': 1.2976614236831665, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.7}\n",
      "{'loss': 0.6953, 'grad_norm': 1.1311142444610596, 'learning_rate': 1.8e-05, 'epoch': 1.8}\n",
      "{'loss': 0.6829, 'grad_norm': 1.303282380104065, 'learning_rate': 1.9e-05, 'epoch': 1.9}\n",
      "{'loss': 0.7181, 'grad_norm': 1.7442694902420044, 'learning_rate': 2e-05, 'epoch': 2.0}\n",
      "{'loss': 0.6917, 'grad_norm': 3.002049446105957, 'learning_rate': 2.1e-05, 'epoch': 2.1}\n",
      "{'loss': 0.6848, 'grad_norm': 2.052790641784668, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.2}\n",
      "{'loss': 0.6893, 'grad_norm': 3.4555671215057373, 'learning_rate': 2.3000000000000003e-05, 'epoch': 2.3}\n",
      "{'loss': 0.6989, 'grad_norm': 2.0745902061462402, 'learning_rate': 2.4e-05, 'epoch': 2.4}\n",
      "{'loss': 0.7031, 'grad_norm': 5.205079078674316, 'learning_rate': 2.5e-05, 'epoch': 2.5}\n",
      "{'loss': 0.7049, 'grad_norm': 2.8923535346984863, 'learning_rate': 2.6000000000000002e-05, 'epoch': 2.6}\n",
      "{'loss': 0.7057, 'grad_norm': 2.337315320968628, 'learning_rate': 2.7000000000000002e-05, 'epoch': 2.7}\n",
      "{'loss': 0.6935, 'grad_norm': 1.8512178659439087, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.8}\n",
      "{'loss': 0.6825, 'grad_norm': 3.6252315044403076, 'learning_rate': 2.9e-05, 'epoch': 2.9}\n",
      "{'loss': 0.6856, 'grad_norm': 2.7927603721618652, 'learning_rate': 3e-05, 'epoch': 3.0}\n",
      "{'train_runtime': 153.7467, 'train_samples_per_second': 15.591, 'train_steps_per_second': 1.951, 'train_loss': 0.6957423639297485, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.6957423639297485, metrics={'train_runtime': 153.7467, 'train_samples_per_second': 15.591, 'train_steps_per_second': 1.951, 'total_flos': 317524354578432.0, 'train_loss': 0.6957423639297485, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained('distilroberta-base', num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:14<00:00,  1.67it/s]\n",
      "/opt/homebrew/anaconda3/envs/DistributedComputing/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 25/25 [00:14<00:00,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Model - Accuracy: 0.545, F1 Score: 0.3357664233576642\n",
      "Pre-trained Model - Accuracy: 0.49, F1 Score: 0.6577181208053692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "finetuned_model = model\n",
    "finetuned_accuracy, finetuned_f1 = evaluate_model(finetuned_model, val_dataset)\n",
    "\n",
    "pretrained_model = RobertaForSequenceClassification.from_pretrained('distilroberta-base', num_labels=2)\n",
    "pretrained_accuracy, pretrained_f1 = evaluate_model(pretrained_model, val_dataset)\n",
    "\n",
    "print(f'Fine-tuned Model - Accuracy: {finetuned_accuracy}, F1 Score: {finetuned_f1}')\n",
    "print(f'Pre-trained Model - Accuracy: {pretrained_accuracy}, F1 Score: {pretrained_f1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/DistributedComputing/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e6faab050d4fba895e920ee5245768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6945, 'grad_norm': 0.15243421494960785, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.1}\n",
      "{'loss': 0.6931, 'grad_norm': 0.15644918382167816, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.2}\n",
      "{'loss': 0.696, 'grad_norm': 0.12537923455238342, 'learning_rate': 3e-06, 'epoch': 0.3}\n",
      "{'loss': 0.6914, 'grad_norm': 0.16847458481788635, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.4}\n",
      "{'loss': 0.6838, 'grad_norm': 0.22018037736415863, 'learning_rate': 5e-06, 'epoch': 0.5}\n",
      "{'loss': 0.6901, 'grad_norm': 0.1848909705877304, 'learning_rate': 6e-06, 'epoch': 0.6}\n",
      "{'loss': 0.6942, 'grad_norm': 0.29606980085372925, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.7}\n",
      "{'loss': 0.7001, 'grad_norm': 0.3684787154197693, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.8}\n",
      "{'loss': 0.6931, 'grad_norm': 0.28186720609664917, 'learning_rate': 9e-06, 'epoch': 0.9}\n",
      "{'loss': 0.6878, 'grad_norm': 0.1738651543855667, 'learning_rate': 1e-05, 'epoch': 1.0}\n",
      "{'loss': 0.6935, 'grad_norm': 0.1649654060602188, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.1}\n",
      "{'loss': 0.6835, 'grad_norm': 0.16026712954044342, 'learning_rate': 1.2e-05, 'epoch': 1.2}\n",
      "{'loss': 0.6892, 'grad_norm': 0.12396804988384247, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.3}\n",
      "{'loss': 0.6931, 'grad_norm': 0.47269493341445923, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.4}\n",
      "{'loss': 0.701, 'grad_norm': 0.47603946924209595, 'learning_rate': 1.5e-05, 'epoch': 1.5}\n",
      "{'loss': 0.6973, 'grad_norm': 0.3060757517814636, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.6}\n",
      "{'loss': 0.6948, 'grad_norm': 0.15119841694831848, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.7}\n",
      "{'loss': 0.6863, 'grad_norm': 0.201408252120018, 'learning_rate': 1.8e-05, 'epoch': 1.8}\n",
      "{'loss': 0.6956, 'grad_norm': 0.16078586876392365, 'learning_rate': 1.9e-05, 'epoch': 1.9}\n",
      "{'loss': 0.6923, 'grad_norm': 0.28573504090309143, 'learning_rate': 2e-05, 'epoch': 2.0}\n",
      "{'loss': 0.6915, 'grad_norm': 0.1518913060426712, 'learning_rate': 2.1e-05, 'epoch': 2.1}\n",
      "{'loss': 0.6807, 'grad_norm': 0.2195996791124344, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.2}\n",
      "{'loss': 0.689, 'grad_norm': 0.32219064235687256, 'learning_rate': 2.3000000000000003e-05, 'epoch': 2.3}\n",
      "{'loss': 0.6906, 'grad_norm': 0.18397267162799835, 'learning_rate': 2.4e-05, 'epoch': 2.4}\n",
      "{'loss': 0.6864, 'grad_norm': 0.3060779869556427, 'learning_rate': 2.5e-05, 'epoch': 2.5}\n",
      "{'loss': 0.6979, 'grad_norm': 0.25404295325279236, 'learning_rate': 2.6000000000000002e-05, 'epoch': 2.6}\n",
      "{'loss': 0.7002, 'grad_norm': 0.22082503139972687, 'learning_rate': 2.7000000000000002e-05, 'epoch': 2.7}\n",
      "{'loss': 0.6861, 'grad_norm': 0.19081729650497437, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.8}\n",
      "{'loss': 0.6912, 'grad_norm': 0.2660275399684906, 'learning_rate': 2.9e-05, 'epoch': 2.9}\n",
      "{'loss': 0.6967, 'grad_norm': 0.2386406511068344, 'learning_rate': 3e-05, 'epoch': 3.0}\n",
      "{'train_runtime': 124.7526, 'train_samples_per_second': 19.214, 'train_steps_per_second': 2.405, 'train_loss': 0.6920373010635376, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:16<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Model - Accuracy: 0.51, F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lora_model = RobertaForSequenceClassification.from_pretrained('distilroberta-base', num_labels=2)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules= [\"attention.self.query\", \"attention.self.key\", \"attention.self.value\"], #[\"query\", \"key\", \"value\"],  # Specify the target modules\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(lora_model, lora_config)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "lora_finetuned_model = lora_model\n",
    "lora_finetuned_accuracy, lora_finetuned_f1 = evaluate_model(lora_finetuned_model, val_dataset)\n",
    "\n",
    "print(f'Fine-tuned Model - Accuracy: {lora_finetuned_accuracy}, F1 Score: {lora_finetuned_f1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
