{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOps Final Project:\n",
    "\n",
    "Comparing performance of fine-tuned LLM vs out of the box pretrained LLM on sentiment analysis task for IMDB reviews dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/DistributedComputing/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4cacb838380438abaa01d7902543150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/799 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0008988fc4a47d684b52949a68a44aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/z7/50r70c7n2klfyy8qsv4fs97c0000gn/T/ipykernel_99401/35699129.py:48: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n",
      "/opt/homebrew/anaconda3/envs/DistributedComputing/lib/python3.11/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.2/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a355ad9a1a456fb9b3e417b4ddb564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f300bb89b054ea7a8595c80b3143dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7016, 'grad_norm': 1.2517576217651367, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.1}\n",
      "{'loss': 0.6893, 'grad_norm': 1.4068324565887451, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.2}\n",
      "{'loss': 0.6914, 'grad_norm': 1.2125697135925293, 'learning_rate': 3e-06, 'epoch': 0.3}\n",
      "{'loss': 0.6856, 'grad_norm': 2.0932648181915283, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.4}\n",
      "{'loss': 0.6793, 'grad_norm': 1.796614170074463, 'learning_rate': 5e-06, 'epoch': 0.5}\n",
      "{'loss': 0.6942, 'grad_norm': 1.592820405960083, 'learning_rate': 6e-06, 'epoch': 0.6}\n",
      "{'loss': 0.6838, 'grad_norm': 2.9470982551574707, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.7}\n",
      "{'loss': 0.6581, 'grad_norm': 2.7445967197418213, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.8}\n",
      "{'loss': 0.6594, 'grad_norm': 3.3230276107788086, 'learning_rate': 9e-06, 'epoch': 0.9}\n",
      "{'loss': 0.5784, 'grad_norm': 3.5184383392333984, 'learning_rate': 1e-05, 'epoch': 1.0}\n",
      "{'loss': 0.5139, 'grad_norm': 4.003285884857178, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.1}\n",
      "{'loss': 0.427, 'grad_norm': 7.641271114349365, 'learning_rate': 1.2e-05, 'epoch': 1.2}\n",
      "{'loss': 0.3596, 'grad_norm': 13.698860168457031, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.3}\n",
      "{'loss': 0.3963, 'grad_norm': 7.904320240020752, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.4}\n",
      "{'loss': 0.2891, 'grad_norm': 15.507389068603516, 'learning_rate': 1.5e-05, 'epoch': 1.5}\n",
      "{'loss': 0.3201, 'grad_norm': 11.332734107971191, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.6}\n",
      "{'loss': 0.3213, 'grad_norm': 33.21695327758789, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.7}\n",
      "{'loss': 0.4134, 'grad_norm': 30.61336326599121, 'learning_rate': 1.8e-05, 'epoch': 1.8}\n",
      "{'loss': 0.5738, 'grad_norm': 8.793739318847656, 'learning_rate': 1.9e-05, 'epoch': 1.9}\n",
      "{'loss': 0.3009, 'grad_norm': 4.938709259033203, 'learning_rate': 2e-05, 'epoch': 2.0}\n",
      "{'loss': 0.1304, 'grad_norm': 0.6481224298477173, 'learning_rate': 2.1e-05, 'epoch': 2.1}\n",
      "{'loss': 0.1869, 'grad_norm': 21.643077850341797, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.2}\n",
      "{'loss': 0.1175, 'grad_norm': 15.19360637664795, 'learning_rate': 2.3000000000000003e-05, 'epoch': 2.3}\n",
      "{'loss': 0.1828, 'grad_norm': 0.22952266037464142, 'learning_rate': 2.4e-05, 'epoch': 2.4}\n",
      "{'loss': 0.194, 'grad_norm': 1.8881282806396484, 'learning_rate': 2.5e-05, 'epoch': 2.5}\n",
      "{'loss': 0.5113, 'grad_norm': 38.62336349487305, 'learning_rate': 2.6000000000000002e-05, 'epoch': 2.6}\n",
      "{'loss': 0.7378, 'grad_norm': 8.581001281738281, 'learning_rate': 2.7000000000000002e-05, 'epoch': 2.7}\n",
      "{'loss': 0.1043, 'grad_norm': 0.26994258165359497, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.8}\n",
      "{'loss': 0.3219, 'grad_norm': 0.18210038542747498, 'learning_rate': 2.9e-05, 'epoch': 2.9}\n",
      "{'loss': 0.2135, 'grad_norm': 46.793067932128906, 'learning_rate': 3e-05, 'epoch': 3.0}\n",
      "{'train_runtime': 175.2828, 'train_samples_per_second': 13.675, 'train_steps_per_second': 1.712, 'train_loss': 0.4445688744386037, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d18756ee21147ec82925be6542de63b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "argmax(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m eval_result \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(eval_result)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DistributedComputing/lib/python3.11/site-packages/transformers/trainer.py:3467\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3464\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3466\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3467\u001b[0m output \u001b[38;5;241m=\u001b[39m eval_loop(\n\u001b[1;32m   3468\u001b[0m     eval_dataloader,\n\u001b[1;32m   3469\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3470\u001b[0m     \u001b[38;5;66;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;00m\n\u001b[1;32m   3471\u001b[0m     \u001b[38;5;66;03m# self.args.prediction_loss_only\u001b[39;00m\n\u001b[1;32m   3472\u001b[0m     prediction_loss_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   3473\u001b[0m     ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys,\n\u001b[1;32m   3474\u001b[0m     metric_key_prefix\u001b[38;5;241m=\u001b[39mmetric_key_prefix,\n\u001b[1;32m   3475\u001b[0m )\n\u001b[1;32m   3477\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/DistributedComputing/lib/python3.11/site-packages/transformers/trainer.py:3719\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3715\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[1;32m   3716\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[1;32m   3717\u001b[0m         )\n\u001b[1;32m   3718\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3719\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels))\n\u001b[1;32m   3720\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3721\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[1], line 52\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(eval_pred)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_metrics\u001b[39m(eval_pred):\n\u001b[1;32m     51\u001b[0m     logits, labels \u001b[38;5;241m=\u001b[39m eval_pred\n\u001b[0;32m---> 52\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m metric\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mpredictions, references\u001b[38;5;241m=\u001b[39mlabels)\n",
      "\u001b[0;31mTypeError\u001b[0m: argmax(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, load_metric\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "# Load dataset\n",
    "# dataset = load_dataset('imdb')\n",
    "# train_dataset = dataset['train']\n",
    "# test_dataset = dataset['test']\n",
    "df = pd.read_csv('movie_short.csv')\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "\n",
    "# Preprocess the data\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "encoded_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "encoded_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Load model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Define metric\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_train_dataset,\n",
    "    eval_dataset=encoded_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate model\n",
    "eval_result = trainer.evaluate()\n",
    "print(eval_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/DistributedComputing/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ceaaf650dd46fda4860ff22296d9a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6929181814193726, 'eval_accuracy': 0.525, 'eval_f1': 0.31654676258992803, 'eval_precision': 0.5365853658536586, 'eval_recall': 0.22448979591836735, 'eval_runtime': 4.4391, 'eval_samples_per_second': 45.054, 'eval_steps_per_second': 2.929}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b33194d6d84667a80343c511286407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6974, 'grad_norm': 1.4938287734985352, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.2}\n",
      "{'loss': 0.6979, 'grad_norm': 1.3188352584838867, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.4}\n",
      "{'loss': 0.6834, 'grad_norm': 1.2274770736694336, 'learning_rate': 3e-06, 'epoch': 0.6}\n",
      "{'loss': 0.6947, 'grad_norm': 1.697504997253418, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.8}\n",
      "{'loss': 0.6842, 'grad_norm': 0.967103123664856, 'learning_rate': 5e-06, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3fb6928990486fba8957e3064e60d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6836497783660889, 'eval_accuracy': 0.65, 'eval_f1': 0.5138888888888888, 'eval_precision': 0.8043478260869565, 'eval_recall': 0.37755102040816324, 'eval_runtime': 4.4577, 'eval_samples_per_second': 44.866, 'eval_steps_per_second': 2.916, 'epoch': 1.0}\n",
      "{'loss': 0.6825, 'grad_norm': 1.4007461071014404, 'learning_rate': 6e-06, 'epoch': 1.2}\n",
      "{'loss': 0.6692, 'grad_norm': 3.2846662998199463, 'learning_rate': 7.000000000000001e-06, 'epoch': 1.4}\n",
      "{'loss': 0.656, 'grad_norm': 2.2971441745758057, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.6}\n",
      "{'loss': 0.6312, 'grad_norm': 1.886910080909729, 'learning_rate': 9e-06, 'epoch': 1.8}\n",
      "{'loss': 0.5765, 'grad_norm': 2.567556619644165, 'learning_rate': 1e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6df99a7c68e46b799d0670bf1111ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5507641434669495, 'eval_accuracy': 0.78, 'eval_f1': 0.8070175438596491, 'eval_precision': 0.7076923076923077, 'eval_recall': 0.9387755102040817, 'eval_runtime': 4.3776, 'eval_samples_per_second': 45.687, 'eval_steps_per_second': 2.97, 'epoch': 2.0}\n",
      "{'loss': 0.4901, 'grad_norm': 2.909862518310547, 'learning_rate': 1.1000000000000001e-05, 'epoch': 2.2}\n",
      "{'loss': 0.369, 'grad_norm': 3.7406013011932373, 'learning_rate': 1.2e-05, 'epoch': 2.4}\n",
      "{'loss': 0.3643, 'grad_norm': 8.123082160949707, 'learning_rate': 1.3000000000000001e-05, 'epoch': 2.6}\n",
      "{'loss': 0.3566, 'grad_norm': 9.323834419250488, 'learning_rate': 1.4000000000000001e-05, 'epoch': 2.8}\n",
      "{'loss': 0.2799, 'grad_norm': 5.017178535461426, 'learning_rate': 1.5e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83863d5331914efdbb2902814f1715f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30060872435569763, 'eval_accuracy': 0.89, 'eval_f1': 0.8829787234042553, 'eval_precision': 0.9222222222222223, 'eval_recall': 0.8469387755102041, 'eval_runtime': 4.3009, 'eval_samples_per_second': 46.501, 'eval_steps_per_second': 3.023, 'epoch': 3.0}\n",
      "{'train_runtime': 181.8816, 'train_samples_per_second': 13.179, 'train_steps_per_second': 0.825, 'train_loss': 0.5688720019658406, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c55b379e92e4d4db0b85f05a67304c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30060872435569763, 'eval_accuracy': 0.89, 'eval_f1': 0.8829787234042553, 'eval_precision': 0.9222222222222223, 'eval_recall': 0.8469387755102041, 'eval_runtime': 4.2623, 'eval_samples_per_second': 46.923, 'eval_steps_per_second': 3.05, 'epoch': 3.0}\n",
      "Pre-trained DistilBERT evaluation:\n",
      "{'eval_loss': 0.6929181814193726, 'eval_accuracy': 0.525, 'eval_f1': 0.31654676258992803, 'eval_precision': 0.5365853658536586, 'eval_recall': 0.22448979591836735, 'eval_runtime': 4.4391, 'eval_samples_per_second': 45.054, 'eval_steps_per_second': 2.929}\n",
      "Fine-tuned DistilBERT evaluation:\n",
      "{'eval_loss': 0.30060872435569763, 'eval_accuracy': 0.89, 'eval_f1': 0.8829787234042553, 'eval_precision': 0.9222222222222223, 'eval_recall': 0.8469387755102041, 'eval_runtime': 4.2623, 'eval_samples_per_second': 46.923, 'eval_steps_per_second': 3.05, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, load_metric\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('movie_short.csv')\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "train_encodings = tokenizer(train_df['text'].tolist(), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_df['text'].tolist(), truncation=True, padding=True)\n",
    "\n",
    "# Convert to torch dataset\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, train_df['label'].tolist())\n",
    "val_dataset = IMDbDataset(val_encodings, val_df['label'].tolist())\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Define the evaluation function\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, pred, average='binary')\n",
    "    acc = accuracy_score(labels, pred)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=0,  # No training, just evaluation\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    do_eval=True\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Evaluate the pre-trained model\n",
    "eval_result = trainer.evaluate()\n",
    "print(eval_result)\n",
    "\n",
    "# Training arguments for fine-tuning\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,  # You can adjust this as needed\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Initialize the Trainer for fine-tuning\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the fine-tuned model\n",
    "eval_result_fine_tuned = trainer.evaluate()\n",
    "print(eval_result_fine_tuned)\n",
    "\n",
    "print(\"Pre-trained DistilBERT evaluation:\")\n",
    "print(eval_result)\n",
    "\n",
    "print(\"Fine-tuned DistilBERT evaluation:\")\n",
    "print(eval_result_fine_tuned)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/DistributedComputing/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578287ee1db240c1a812e1a1dc1e7c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8fabdbce6d94aedad10ec7c4130049c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6845, 'grad_norm': 1.402540922164917, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.2}\n",
      "{'loss': 0.6878, 'grad_norm': 1.3427975177764893, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.4}\n",
      "{'loss': 0.6826, 'grad_norm': 1.4069232940673828, 'learning_rate': 3e-06, 'epoch': 0.6}\n",
      "{'loss': 0.6958, 'grad_norm': 1.7702349424362183, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.8}\n",
      "{'loss': 0.6714, 'grad_norm': 1.014061689376831, 'learning_rate': 5e-06, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c11ca331dde45f88b385e30bb4c7b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6795913577079773, 'eval_accuracy': 0.655, 'eval_f1': 0.46511627906976744, 'eval_precision': 0.967741935483871, 'eval_recall': 0.30612244897959184, 'eval_runtime': 4.4483, 'eval_samples_per_second': 44.961, 'eval_steps_per_second': 2.922, 'epoch': 1.0}\n",
      "{'train_runtime': 62.0851, 'train_samples_per_second': 12.869, 'train_steps_per_second': 0.805, 'train_loss': 0.6844189453125, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db7a4ba1866460f9160f6bb60a935ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/DistributedComputing/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6bd41886be4b0fb0b6dd0bafc1b1dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.67, 'grad_norm': 1.5083074569702148, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.2}\n",
      "{'loss': 0.6747, 'grad_norm': 1.4883174896240234, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.4}\n",
      "{'loss': 0.6623, 'grad_norm': 1.5165575742721558, 'learning_rate': 3e-06, 'epoch': 0.6}\n",
      "{'loss': 0.6727, 'grad_norm': 1.9634284973144531, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.8}\n",
      "{'loss': 0.6407, 'grad_norm': 1.265491008758545, 'learning_rate': 5e-06, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86f09f1820d449c89ac22358300016c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.648670494556427, 'eval_accuracy': 0.805, 'eval_f1': 0.7936507936507936, 'eval_precision': 0.8241758241758241, 'eval_recall': 0.7653061224489796, 'eval_runtime': 4.4635, 'eval_samples_per_second': 44.808, 'eval_steps_per_second': 2.913, 'epoch': 1.0}\n",
      "{'train_runtime': 61.1212, 'train_samples_per_second': 13.072, 'train_steps_per_second': 0.818, 'train_loss': 0.6640888595581055, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ac2de0ea5a74564af80b3c4dc72d4c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained DistilBERT evaluation:\n",
      "{'eval_loss': 0.690392017364502, 'eval_accuracy': 0.52, 'eval_f1': 0.04, 'eval_precision': 1.0, 'eval_recall': 0.02040816326530612, 'eval_runtime': 4.4976, 'eval_samples_per_second': 44.468, 'eval_steps_per_second': 2.89}\n",
      "Regular fine-tuned DistilBERT evaluation:\n",
      "{'eval_loss': 0.6795913577079773, 'eval_accuracy': 0.655, 'eval_f1': 0.46511627906976744, 'eval_precision': 0.967741935483871, 'eval_recall': 0.30612244897959184, 'eval_runtime': 4.3478, 'eval_samples_per_second': 46.0, 'eval_steps_per_second': 2.99, 'epoch': 1.0}\n",
      "LoRA fine-tuned DistilBERT evaluation:\n",
      "{'eval_loss': 0.648670494556427, 'eval_accuracy': 0.805, 'eval_f1': 0.7936507936507936, 'eval_precision': 0.8241758241758241, 'eval_recall': 0.7653061224489796, 'eval_runtime': 4.3584, 'eval_samples_per_second': 45.889, 'eval_steps_per_second': 2.983, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "import loralib as lora\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('movie_short.csv')\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "train_encodings = tokenizer(train_df['text'].tolist(), truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_df['text'].tolist(), truncation=True, padding=True)\n",
    "\n",
    "# Convert to torch dataset\n",
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, train_df['label'].tolist())\n",
    "val_dataset = IMDbDataset(val_encodings, val_df['label'].tolist())\n",
    "\n",
    "# Define the evaluation function\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, pred, average='binary')\n",
    "    acc = accuracy_score(labels, pred)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Load the pre-trained DistilBERT model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Evaluate the pre-trained model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=0,  # No training, just evaluation\n",
    "    per_device_eval_batch_size=16,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    do_eval=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "eval_result = trainer.evaluate()\n",
    "\n",
    "# Fine-tune the DistilBERT model without LoRA\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "eval_result_fine_tuned = trainer.evaluate()\n",
    "\n",
    "\n",
    "lora_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules= [\"attention.q_lin\", \"attention.k_lin\", \"attention.v_lin\"], #[\"query\", \"key\", \"value\"],  # Specify the target modules\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(lora_model, lora_config)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "eval_result_lora = trainer.evaluate()\n",
    "\n",
    "\n",
    "# # Fine-tune the DistilBERT model with LoRA\n",
    "# class LoRADistilBertForSequenceClassification(DistilBertForSequenceClassification):\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "#         lora.apply_lora(self)\n",
    "\n",
    "# # Load and wrap the DistilBERT model with LoRA\n",
    "# lora_model = LoRADistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=lora_model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )\n",
    "\n",
    "# trainer.train()\n",
    "\n",
    "# eval_result_lora_fine_tuned = trainer.evaluate()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Pre-trained DistilBERT evaluation:\")\n",
    "print(eval_result)\n",
    "\n",
    "print(\"Regular fine-tuned DistilBERT evaluation:\")\n",
    "print(eval_result_fine_tuned)\n",
    "\n",
    "print(\"LoRA fine-tuned DistilBERT evaluation:\")\n",
    "print(eval_result_lora)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/DistributedComputing/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627a5745cdc841d68c0c75350cea01de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/799 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f8f8f5c6ec4371b2e5595fb4a93fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "df = pd.read_csv('movie_short.csv')\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilBERT Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced38498a2f74bd5912f9d3e448a073d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6878, 'grad_norm': 1.3967981338500977, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.1}\n",
      "{'loss': 0.6949, 'grad_norm': 1.4446425437927246, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.2}\n",
      "{'loss': 0.698, 'grad_norm': 1.2860491275787354, 'learning_rate': 3e-06, 'epoch': 0.3}\n",
      "{'loss': 0.6927, 'grad_norm': 1.5432770252227783, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.4}\n",
      "{'loss': 0.6645, 'grad_norm': 1.4422451257705688, 'learning_rate': 5e-06, 'epoch': 0.5}\n",
      "{'loss': 0.7013, 'grad_norm': 1.577475666999817, 'learning_rate': 6e-06, 'epoch': 0.6}\n",
      "{'loss': 0.7076, 'grad_norm': 2.1963300704956055, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.7}\n",
      "{'loss': 0.6862, 'grad_norm': 2.656816244125366, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.8}\n",
      "{'loss': 0.6699, 'grad_norm': 2.4682140350341797, 'learning_rate': 9e-06, 'epoch': 0.9}\n",
      "{'loss': 0.6375, 'grad_norm': 1.9719682931900024, 'learning_rate': 1e-05, 'epoch': 1.0}\n",
      "{'loss': 0.6135, 'grad_norm': 2.478562355041504, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.1}\n",
      "{'loss': 0.5193, 'grad_norm': 3.697397470474243, 'learning_rate': 1.2e-05, 'epoch': 1.2}\n",
      "{'loss': 0.4571, 'grad_norm': 9.954147338867188, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.3}\n",
      "{'loss': 0.4609, 'grad_norm': 9.038082122802734, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.4}\n",
      "{'loss': 0.3698, 'grad_norm': 15.671046257019043, 'learning_rate': 1.5e-05, 'epoch': 1.5}\n",
      "{'loss': 0.3681, 'grad_norm': 13.168142318725586, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.6}\n",
      "{'loss': 0.3665, 'grad_norm': 19.74769401550293, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.7}\n",
      "{'loss': 0.4138, 'grad_norm': 11.957159042358398, 'learning_rate': 1.8e-05, 'epoch': 1.8}\n",
      "{'loss': 0.4197, 'grad_norm': 6.91489839553833, 'learning_rate': 1.9e-05, 'epoch': 1.9}\n",
      "{'loss': 0.2858, 'grad_norm': 5.032744407653809, 'learning_rate': 2e-05, 'epoch': 2.0}\n",
      "{'loss': 0.1493, 'grad_norm': 0.9111071825027466, 'learning_rate': 2.1e-05, 'epoch': 2.1}\n",
      "{'loss': 0.1674, 'grad_norm': 5.2817606925964355, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.2}\n",
      "{'loss': 0.09, 'grad_norm': 0.296331524848938, 'learning_rate': 2.3000000000000003e-05, 'epoch': 2.3}\n",
      "{'loss': 0.2398, 'grad_norm': 21.335786819458008, 'learning_rate': 2.4e-05, 'epoch': 2.4}\n",
      "{'loss': 0.2327, 'grad_norm': 4.788655757904053, 'learning_rate': 2.5e-05, 'epoch': 2.5}\n",
      "{'loss': 0.3357, 'grad_norm': 34.423553466796875, 'learning_rate': 2.6000000000000002e-05, 'epoch': 2.6}\n",
      "{'loss': 0.5907, 'grad_norm': 36.60173797607422, 'learning_rate': 2.7000000000000002e-05, 'epoch': 2.7}\n",
      "{'loss': 0.1527, 'grad_norm': 6.965003490447998, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.8}\n",
      "{'loss': 0.3642, 'grad_norm': 0.6126039028167725, 'learning_rate': 2.9e-05, 'epoch': 2.9}\n",
      "{'loss': 0.1544, 'grad_norm': 24.536083221435547, 'learning_rate': 3e-05, 'epoch': 3.0}\n",
      "{'train_runtime': 177.3973, 'train_samples_per_second': 13.512, 'train_steps_per_second': 1.691, 'train_loss': 0.4530573260784149, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.4530573260784149, metrics={'train_runtime': 177.3973, 'train_samples_per_second': 13.512, 'train_steps_per_second': 1.691, 'total_flos': 317524354578432.0, 'train_loss': 0.4530573260784149, 'epoch': 3.0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaulating DistilBERT fine-tuned vs pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:15<00:00,  1.60it/s]\n",
      "/opt/homebrew/anaconda3/envs/DistributedComputing/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:15<00:00,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Model - Accuracy: 0.875, F1 Score: 0.8663101604278075\n",
      "Pre-trained Model - Accuracy: 0.51, F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm  # Use standard tqdm\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataset, batch_size=8):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    preds = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):  \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels.extend(batch['label'].cpu().numpy())\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    return accuracy, f1\n",
    "\n",
    "finetuned_model = model\n",
    "finetuned_accuracy, finetuned_f1 = evaluate_model(finetuned_model, val_dataset)\n",
    "\n",
    "pretrained_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "pretrained_accuracy, pretrained_f1 = evaluate_model(pretrained_model, val_dataset)\n",
    "\n",
    "print(f'Fine-tuned Model - Accuracy: {finetuned_accuracy}, F1 Score: {finetuned_f1}')\n",
    "print(f'Pre-trained Model - Accuracy: {pretrained_accuracy}, F1 Score: {pretrained_f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning with LoRA instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/DistributedComputing/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202c3bd4922144059a4a660b4e9eaa5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7221, 'grad_norm': 0.10963136702775955, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.1}\n",
      "{'loss': 0.7116, 'grad_norm': 0.1270284503698349, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.2}\n",
      "{'loss': 0.706, 'grad_norm': 0.10134343057870865, 'learning_rate': 3e-06, 'epoch': 0.3}\n",
      "{'loss': 0.7136, 'grad_norm': 0.1646464467048645, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.4}\n",
      "{'loss': 0.7533, 'grad_norm': 0.15595752000808716, 'learning_rate': 5e-06, 'epoch': 0.5}\n",
      "{'loss': 0.7051, 'grad_norm': 0.11388856917619705, 'learning_rate': 6e-06, 'epoch': 0.6}\n",
      "{'loss': 0.7031, 'grad_norm': 0.21470071375370026, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.7}\n",
      "{'loss': 0.6796, 'grad_norm': 0.13678190112113953, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.8}\n",
      "{'loss': 0.7245, 'grad_norm': 0.1396872103214264, 'learning_rate': 9e-06, 'epoch': 0.9}\n",
      "{'loss': 0.7288, 'grad_norm': 0.164378821849823, 'learning_rate': 1e-05, 'epoch': 1.0}\n",
      "{'loss': 0.7124, 'grad_norm': 0.12946459650993347, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.1}\n",
      "{'loss': 0.7122, 'grad_norm': 0.12616731226444244, 'learning_rate': 1.2e-05, 'epoch': 1.2}\n",
      "{'loss': 0.7205, 'grad_norm': 0.13493697345256805, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.3}\n",
      "{'loss': 0.738, 'grad_norm': 0.4659038782119751, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.4}\n",
      "{'loss': 0.719, 'grad_norm': 0.2536369860172272, 'learning_rate': 1.5e-05, 'epoch': 1.5}\n",
      "{'loss': 0.6887, 'grad_norm': 0.1932983249425888, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.6}\n",
      "{'loss': 0.7127, 'grad_norm': 0.1408839225769043, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.7}\n",
      "{'loss': 0.7147, 'grad_norm': 0.15335386991500854, 'learning_rate': 1.8e-05, 'epoch': 1.8}\n",
      "{'loss': 0.7098, 'grad_norm': 0.14273470640182495, 'learning_rate': 1.9e-05, 'epoch': 1.9}\n",
      "{'loss': 0.7027, 'grad_norm': 0.25989967584609985, 'learning_rate': 2e-05, 'epoch': 2.0}\n",
      "{'loss': 0.7008, 'grad_norm': 0.16816329956054688, 'learning_rate': 2.1e-05, 'epoch': 2.1}\n",
      "{'loss': 0.7014, 'grad_norm': 0.16126461327075958, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.2}\n",
      "{'loss': 0.7002, 'grad_norm': 0.38778114318847656, 'learning_rate': 2.3000000000000003e-05, 'epoch': 2.3}\n",
      "{'loss': 0.6992, 'grad_norm': 0.17109012603759766, 'learning_rate': 2.4e-05, 'epoch': 2.4}\n",
      "{'loss': 0.6999, 'grad_norm': 0.3580131530761719, 'learning_rate': 2.5e-05, 'epoch': 2.5}\n",
      "{'loss': 0.681, 'grad_norm': 0.291911244392395, 'learning_rate': 2.6000000000000002e-05, 'epoch': 2.6}\n",
      "{'loss': 0.7016, 'grad_norm': 0.2238336205482483, 'learning_rate': 2.7000000000000002e-05, 'epoch': 2.7}\n",
      "{'loss': 0.6923, 'grad_norm': 0.2899482548236847, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.8}\n",
      "{'loss': 0.7007, 'grad_norm': 0.4326309263706207, 'learning_rate': 2.9e-05, 'epoch': 2.9}\n",
      "{'loss': 0.6914, 'grad_norm': 0.48042505979537964, 'learning_rate': 3e-05, 'epoch': 3.0}\n",
      "{'train_runtime': 150.7727, 'train_samples_per_second': 15.898, 'train_steps_per_second': 1.99, 'train_loss': 0.7082284053166708, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:17<00:00,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Model - Accuracy: 0.645, F1 Score: 0.6926406926406926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules= [\"attention.q_lin\", \"attention.k_lin\", \"attention.v_lin\"], #[\"query\", \"key\", \"value\"],  # Specify the target modules\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(lora_model, lora_config)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "lora_finetuned_model = lora_model\n",
    "lora_finetuned_accuracy, lora_finetuned_f1 = evaluate_model(lora_finetuned_model, val_dataset)\n",
    "\n",
    "print(f'Fine-tuned Model - Accuracy: {lora_finetuned_accuracy}, F1 Score: {lora_finetuned_f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same process as above but on DistillRoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/DistributedComputing/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7978378f994593b4a01b747feb3767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6999, 'grad_norm': 1.3030779361724854, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.1}\n",
      "{'loss': 0.7006, 'grad_norm': 2.0900352001190186, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.2}\n",
      "{'loss': 0.6983, 'grad_norm': 1.2767045497894287, 'learning_rate': 3e-06, 'epoch': 0.3}\n",
      "{'loss': 0.6956, 'grad_norm': 1.710085391998291, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.4}\n",
      "{'loss': 0.6823, 'grad_norm': 1.9790459871292114, 'learning_rate': 5e-06, 'epoch': 0.5}\n",
      "{'loss': 0.7201, 'grad_norm': 1.8730825185775757, 'learning_rate': 6e-06, 'epoch': 0.6}\n",
      "{'loss': 0.7134, 'grad_norm': 3.2479703426361084, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.7}\n",
      "{'loss': 0.7103, 'grad_norm': 3.5436835289001465, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.8}\n",
      "{'loss': 0.6854, 'grad_norm': 2.443458318710327, 'learning_rate': 9e-06, 'epoch': 0.9}\n",
      "{'loss': 0.6914, 'grad_norm': 2.5282208919525146, 'learning_rate': 1e-05, 'epoch': 1.0}\n",
      "{'loss': 0.692, 'grad_norm': 2.087980270385742, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.1}\n",
      "{'loss': 0.6833, 'grad_norm': 1.7888507843017578, 'learning_rate': 1.2e-05, 'epoch': 1.2}\n",
      "{'loss': 0.6823, 'grad_norm': 1.3680816888809204, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.3}\n",
      "{'loss': 0.6848, 'grad_norm': 7.204034805297852, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.4}\n",
      "{'loss': 0.703, 'grad_norm': 6.5707244873046875, 'learning_rate': 1.5e-05, 'epoch': 1.5}\n",
      "{'loss': 0.6937, 'grad_norm': 3.301345109939575, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.6}\n",
      "{'loss': 0.6994, 'grad_norm': 1.297660231590271, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.7}\n",
      "{'loss': 0.6953, 'grad_norm': 1.1311140060424805, 'learning_rate': 1.8e-05, 'epoch': 1.8}\n",
      "{'loss': 0.6829, 'grad_norm': 1.3032814264297485, 'learning_rate': 1.9e-05, 'epoch': 1.9}\n",
      "{'loss': 0.7181, 'grad_norm': 1.7442691326141357, 'learning_rate': 2e-05, 'epoch': 2.0}\n",
      "{'loss': 0.6917, 'grad_norm': 3.0020499229431152, 'learning_rate': 2.1e-05, 'epoch': 2.1}\n",
      "{'loss': 0.6848, 'grad_norm': 2.052790641784668, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.2}\n",
      "{'loss': 0.6893, 'grad_norm': 3.455566167831421, 'learning_rate': 2.3000000000000003e-05, 'epoch': 2.3}\n",
      "{'loss': 0.6989, 'grad_norm': 2.074589490890503, 'learning_rate': 2.4e-05, 'epoch': 2.4}\n",
      "{'loss': 0.7031, 'grad_norm': 5.205077171325684, 'learning_rate': 2.5e-05, 'epoch': 2.5}\n",
      "{'loss': 0.7049, 'grad_norm': 2.8923556804656982, 'learning_rate': 2.6000000000000002e-05, 'epoch': 2.6}\n",
      "{'loss': 0.7057, 'grad_norm': 2.3373098373413086, 'learning_rate': 2.7000000000000002e-05, 'epoch': 2.7}\n",
      "{'loss': 0.6935, 'grad_norm': 1.8512208461761475, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.8}\n",
      "{'loss': 0.6825, 'grad_norm': 3.625230312347412, 'learning_rate': 2.9e-05, 'epoch': 2.9}\n",
      "{'loss': 0.6856, 'grad_norm': 2.7927603721618652, 'learning_rate': 3e-05, 'epoch': 3.0}\n",
      "{'train_runtime': 164.4032, 'train_samples_per_second': 14.58, 'train_steps_per_second': 1.825, 'train_loss': 0.6957423591613769, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.6957423591613769, metrics={'train_runtime': 164.4032, 'train_samples_per_second': 14.58, 'train_steps_per_second': 1.825, 'total_flos': 317524354578432.0, 'train_loss': 0.6957423591613769, 'epoch': 3.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained('distilroberta-base', num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:15<00:00,  1.57it/s]\n",
      "/opt/homebrew/anaconda3/envs/DistributedComputing/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:15<00:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Model - Accuracy: 0.545, F1 Score: 0.3357664233576642\n",
      "Pre-trained Model - Accuracy: 0.49, F1 Score: 0.6577181208053692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "finetuned_model = model\n",
    "finetuned_accuracy, finetuned_f1 = evaluate_model(finetuned_model, val_dataset)\n",
    "\n",
    "pretrained_model = RobertaForSequenceClassification.from_pretrained('distilroberta-base', num_labels=2)\n",
    "pretrained_accuracy, pretrained_f1 = evaluate_model(pretrained_model, val_dataset)\n",
    "\n",
    "print(f'Fine-tuned Model - Accuracy: {finetuned_accuracy}, F1 Score: {finetuned_f1}')\n",
    "print(f'Pre-trained Model - Accuracy: {pretrained_accuracy}, F1 Score: {pretrained_f1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/DistributedComputing/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b41606f2e2d14447b967eddbb08ab58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6945, 'grad_norm': 0.15243421494960785, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.1}\n",
      "{'loss': 0.6931, 'grad_norm': 0.15644918382167816, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.2}\n",
      "{'loss': 0.696, 'grad_norm': 0.12537923455238342, 'learning_rate': 3e-06, 'epoch': 0.3}\n",
      "{'loss': 0.6914, 'grad_norm': 0.16847458481788635, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.4}\n",
      "{'loss': 0.6838, 'grad_norm': 0.22018037736415863, 'learning_rate': 5e-06, 'epoch': 0.5}\n",
      "{'loss': 0.6901, 'grad_norm': 0.1848909705877304, 'learning_rate': 6e-06, 'epoch': 0.6}\n",
      "{'loss': 0.6942, 'grad_norm': 0.29606980085372925, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.7}\n",
      "{'loss': 0.7001, 'grad_norm': 0.3684787154197693, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.8}\n",
      "{'loss': 0.6931, 'grad_norm': 0.28186720609664917, 'learning_rate': 9e-06, 'epoch': 0.9}\n",
      "{'loss': 0.6878, 'grad_norm': 0.1738651543855667, 'learning_rate': 1e-05, 'epoch': 1.0}\n",
      "{'loss': 0.6935, 'grad_norm': 0.1649654060602188, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.1}\n",
      "{'loss': 0.6835, 'grad_norm': 0.16026712954044342, 'learning_rate': 1.2e-05, 'epoch': 1.2}\n",
      "{'loss': 0.6892, 'grad_norm': 0.12396804988384247, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.3}\n",
      "{'loss': 0.6931, 'grad_norm': 0.47269493341445923, 'learning_rate': 1.4000000000000001e-05, 'epoch': 1.4}\n",
      "{'loss': 0.701, 'grad_norm': 0.47603946924209595, 'learning_rate': 1.5e-05, 'epoch': 1.5}\n",
      "{'loss': 0.6973, 'grad_norm': 0.3060757517814636, 'learning_rate': 1.6000000000000003e-05, 'epoch': 1.6}\n",
      "{'loss': 0.6948, 'grad_norm': 0.15119841694831848, 'learning_rate': 1.7000000000000003e-05, 'epoch': 1.7}\n",
      "{'loss': 0.6863, 'grad_norm': 0.201408252120018, 'learning_rate': 1.8e-05, 'epoch': 1.8}\n",
      "{'loss': 0.6956, 'grad_norm': 0.16078586876392365, 'learning_rate': 1.9e-05, 'epoch': 1.9}\n",
      "{'loss': 0.6923, 'grad_norm': 0.28573504090309143, 'learning_rate': 2e-05, 'epoch': 2.0}\n",
      "{'loss': 0.6915, 'grad_norm': 0.1518913060426712, 'learning_rate': 2.1e-05, 'epoch': 2.1}\n",
      "{'loss': 0.6807, 'grad_norm': 0.2195996791124344, 'learning_rate': 2.2000000000000003e-05, 'epoch': 2.2}\n",
      "{'loss': 0.689, 'grad_norm': 0.32219064235687256, 'learning_rate': 2.3000000000000003e-05, 'epoch': 2.3}\n",
      "{'loss': 0.6906, 'grad_norm': 0.18397267162799835, 'learning_rate': 2.4e-05, 'epoch': 2.4}\n",
      "{'loss': 0.6864, 'grad_norm': 0.3060779869556427, 'learning_rate': 2.5e-05, 'epoch': 2.5}\n",
      "{'loss': 0.6979, 'grad_norm': 0.25404295325279236, 'learning_rate': 2.6000000000000002e-05, 'epoch': 2.6}\n",
      "{'loss': 0.7002, 'grad_norm': 0.22082503139972687, 'learning_rate': 2.7000000000000002e-05, 'epoch': 2.7}\n",
      "{'loss': 0.6861, 'grad_norm': 0.19081729650497437, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.8}\n",
      "{'loss': 0.6912, 'grad_norm': 0.2660275399684906, 'learning_rate': 2.9e-05, 'epoch': 2.9}\n",
      "{'loss': 0.6967, 'grad_norm': 0.2386406511068344, 'learning_rate': 3e-05, 'epoch': 3.0}\n",
      "{'train_runtime': 135.3069, 'train_samples_per_second': 17.715, 'train_steps_per_second': 2.217, 'train_loss': 0.6920373010635376, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:17<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned Model - Accuracy: 0.51, F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lora_model = RobertaForSequenceClassification.from_pretrained('distilroberta-base', num_labels=2)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules= [\"attention.self.query\", \"attention.self.key\", \"attention.self.value\"], #[\"query\", \"key\", \"value\"],  # Specify the target modules\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(lora_model, lora_config)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "lora_finetuned_model = lora_model\n",
    "lora_finetuned_accuracy, lora_finetuned_f1 = evaluate_model(lora_finetuned_model, val_dataset)\n",
    "\n",
    "print(f'Fine-tuned Model - Accuracy: {lora_finetuned_accuracy}, F1 Score: {lora_finetuned_f1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
